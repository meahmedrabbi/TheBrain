{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "# TheBrain: PyTorch AI Model Training & Implementation Guide\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/meahmedrabbi/TheBrain/blob/main/TheBrain_PyTorch_Colab.ipynb)\n",
    "\n",
    "This notebook demonstrates how to train and use TheBrain, a conversational AI model built with PyTorch. The model supports:\n",
    "- **Transformer architecture** (attention-based, better for longer contexts)\n",
    "- **LSTM architecture** (recurrent, good for smaller datasets)\n",
    "- **Multiple open datasets** for training\n",
    "- **Easy deployment** and inference\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "First, let's set up the environment and install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install tqdm numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/meahmedrabbi/TheBrain.git\n",
    "%cd TheBrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download and Prepare Multiple Datasets\n",
    "\n",
    "We'll use several open-source conversational datasets:\n",
    "1. **Cornell Movie Dialogs** - Movie character conversations\n",
    "2. **DailyDialog** - Multi-turn daily conversations\n",
    "3. **Custom conversational data** - Included in the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Cornell Movie Dialogs dataset\n",
    "!wget -q http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
    "!unzip -q cornell_movie_dialogs_corpus.zip\n",
    "!mv cornell\\ movie-dialogs\\ corpus cornell_movie_dialogs\n",
    "print(\"\u2713 Cornell Movie Dialogs downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download DailyDialog dataset (sample)\n",
    "# Note: Full dataset requires registration, so we'll create a sample\n",
    "import os\n",
    "\n",
    "# Create sample DailyDialog-style data\n",
    "sample_dialogs = [\n",
    "    \"Good morning! How are you today? __eou__ I'm doing great, thank you! How about you? __eou__ I'm wonderful, thanks for asking.\",\n",
    "    \"What's your favorite food? __eou__ I love pizza! What about you? __eou__ I prefer pasta.\",\n",
    "    \"Have you seen the new movie? __eou__ Yes, I watched it last weekend. It was amazing! __eou__ I heard great reviews about it.\",\n",
    "    \"What are your plans for the weekend? __eou__ I'm going hiking with friends. __eou__ That sounds fun!\",\n",
    "    \"How was your day? __eou__ It was quite busy but productive. __eou__ I'm glad to hear that.\"\n",
    "]\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "with open('data/sample_dailydialog.txt', 'w') as f:\n",
    "    for dialog in sample_dialogs:\n",
    "        f.write(dialog + '\\n')\n",
    "\n",
    "print(\"\u2713 Sample dialog data created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Cornell Movie dataset helper script\n",
    "with open('prepare_cornell_data.py', 'w') as f:\n",
    "    f.write('''import os\n",
    "\n",
    "def prepare_cornell_data():\n",
    "    \"\"\"Convert Cornell Movie dataset to our format\"\"\"\n",
    "    lines_file = 'cornell_movie_dialogs/movie_lines.txt'\n",
    "    conversations_file = 'cornell_movie_dialogs/movie_conversations.txt'\n",
    "    output_file = 'data/cornell_conversations.txt'\n",
    "    \n",
    "    # Load lines\n",
    "    lines = {}\n",
    "    with open(lines_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            parts = line.split(' +++$+++ ')\n",
    "            if len(parts) >= 5:\n",
    "                lines[parts[0]] = parts[4].strip()\n",
    "    \n",
    "    # Create conversation pairs\n",
    "    pairs = []\n",
    "    with open(conversations_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            parts = line.split(' +++$+++ ')\n",
    "            if len(parts) >= 4:\n",
    "                conv = eval(parts[3])\n",
    "                for i in range(len(conv) - 1):\n",
    "                    if conv[i] in lines and conv[i + 1] in lines:\n",
    "                        input_text = lines[conv[i]].replace('|', '')\n",
    "                        output_text = lines[conv[i + 1]].replace('|', '')\n",
    "                        pairs.append(f\"{input_text}|{output_text}\")\n",
    "    \n",
    "    # Save\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for pair in pairs[:5000]:  # Limit to 5000 for faster training\n",
    "            f.write(pair + '\\\\n')\n",
    "    \n",
    "    print(f\"\u2713 Prepared {len(pairs[:5000])} conversation pairs\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    prepare_cornell_data()\n",
    "''')\n",
    "\n",
    "!python prepare_cornell_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available data files\n",
    "!ls -lh conversational_data.txt data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Model\n",
    "\n",
    "Now let's train the model! You can choose between:\n",
    "- **Transformer** (recommended for better performance)\n",
    "- **LSTM** (faster training, good for small datasets)\n",
    "\n",
    "### Option A: Train Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Transformer model on multiple datasets\n",
    "!python train_pytorch.py \\\n",
    "    --model transformer \\\n",
    "    --data conversational_data.txt data/cornell_conversations.txt \\\n",
    "    --epochs 10 \\\n",
    "    --batch_size 32 \\\n",
    "    --lr 0.001 \\\n",
    "    --d_model 256 \\\n",
    "    --max_len 128 \\\n",
    "    --save_dir models \\\n",
    "    --device auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Train LSTM Model (Faster Alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM model (faster, lighter)\n",
    "!python train_pytorch.py \\\n",
    "    --model lstm \\\n",
    "    --data conversational_data.txt data/cornell_conversations.txt \\\n",
    "    --epochs 15 \\\n",
    "    --batch_size 64 \\\n",
    "    --lr 0.001 \\\n",
    "    --d_model 256 \\\n",
    "    --max_len 128 \\\n",
    "    --save_dir models \\\n",
    "    --device auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the Trained Model\n",
    "\n",
    "Let's test our trained model with some example inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with single inputs\n",
    "test_inputs = [\n",
    "    \"hello\",\n",
    "    \"how are you\",\n",
    "    \"what is your name\",\n",
    "    \"tell me about your company\",\n",
    "    \"thank you\"\n",
    "]\n",
    "\n",
    "for text in test_inputs:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Input: {text}\")\n",
    "    !python inference_pytorch.py --model models/best_model.pt --vocab models/vocabulary.json --text \"{text}\" --temperature 0.8\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Chat\n",
    "\n",
    "Start an interactive chat session with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive chat (will work in Colab with input prompts)\n",
    "from inference_pytorch import ChatBot\n",
    "\n",
    "# Initialize chatbot\n",
    "chatbot = ChatBot(\n",
    "    model_path='models/best_model.pt',\n",
    "    vocab_path='models/vocabulary.json',\n",
    "    device='auto'\n",
    ")\n",
    "\n",
    "# Chat interactively\n",
    "print(\"\\nType your message below. Type 'quit' to exit.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    response = chatbot.generate_response(user_input, temperature=0.8)\n",
    "    print(f\"TheBrain: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Download Trained Model\n",
    "\n",
    "Download your trained model to use locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the models directory\n",
    "!zip -r trained_model.zip models/\n",
    "\n",
    "# Download in Colab\n",
    "from google.colab import files\n",
    "files.download('trained_model.zip')\n",
    "\n",
    "print(\"\\n\u2713 Model downloaded! You can now use it locally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Architecture Visualization\n",
    "\n",
    "Let's visualize the model architecture and training statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary\n",
    "import torch\n",
    "from pytorch_model import TransformerModel\n",
    "import json\n",
    "\n",
    "# Load vocabulary\n",
    "with open('models/vocabulary.json', 'r') as f:\n",
    "    vocab_data = json.load(f)\n",
    "\n",
    "vocab_size = vocab_data['vocab_size']\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load('models/best_model.pt', map_location='cpu')\n",
    "\n",
    "print(\"Model Information:\")\n",
    "print(f\"  - Model Type: {checkpoint.get('model_type', 'transformer')}\")\n",
    "print(f\"  - Vocabulary Size: {vocab_size:,}\")\n",
    "print(f\"  - Model Dimension: {checkpoint.get('d_model', 256)}\")\n",
    "print(f\"  - Best Validation Loss: {checkpoint.get('val_loss', 'N/A'):.4f}\")\n",
    "print(f\"  - Trained Epochs: {checkpoint.get('epoch', 'N/A') + 1}\")\n",
    "\n",
    "# Count parameters\n",
    "if checkpoint.get('model_type') == 'transformer':\n",
    "    model = TransformerModel(vocab_size=vocab_size, d_model=checkpoint.get('d_model', 256))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nParameter Count:\")\n",
    "    print(f\"  - Total Parameters: {total_params:,}\")\n",
    "    print(f\"  - Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"  - Model Size: ~{total_params * 4 / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Custom Dataset Integration\n",
    "\n",
    "Here's how to add your own dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create custom dataset\n",
    "custom_data = [\n",
    "    \"What is AI?|Artificial Intelligence is the simulation of human intelligence by machines.\",\n",
    "    \"What is machine learning?|Machine learning is a subset of AI that learns from data.\",\n",
    "    \"What is deep learning?|Deep learning uses neural networks with multiple layers.\",\n",
    "    \"What is PyTorch?|PyTorch is a popular deep learning framework developed by Facebook.\",\n",
    "    \"What is a transformer?|Transformer is an attention-based neural network architecture.\"\n",
    "]\n",
    "\n",
    "# Save custom dataset\n",
    "with open('data/custom_ai_qa.txt', 'w') as f:\n",
    "    for pair in custom_data:\n",
    "        f.write(pair + '\\n')\n",
    "\n",
    "print(\"\u2713 Custom dataset created!\")\n",
    "print(\"\\nTo train with your custom dataset, add it to the --data argument:\")\n",
    "print(\"python train_pytorch.py --data conversational_data.txt data/custom_ai_qa.txt ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Tips for Better Results\n",
    "\n",
    "### Training Tips:\n",
    "1. **More data = better results**: Combine multiple datasets\n",
    "2. **Adjust learning rate**: Try 0.0001 to 0.001\n",
    "3. **Increase model size**: Use `--d_model 512` for better capacity\n",
    "4. **Train longer**: More epochs often help (20-50)\n",
    "5. **Use GPU**: Enable GPU runtime in Colab (Runtime \u2192 Change runtime type \u2192 GPU)\n",
    "\n",
    "### Inference Tips:\n",
    "1. **Temperature**: \n",
    "   - Lower (0.5-0.7) = more focused, deterministic\n",
    "   - Higher (0.9-1.2) = more creative, random\n",
    "2. **Max length**: Adjust based on your use case\n",
    "3. **Fine-tune**: Continue training on domain-specific data\n",
    "\n",
    "### Common Issues:\n",
    "1. **Out of memory**: Reduce `--batch_size` or `--d_model`\n",
    "2. **Poor responses**: Need more training data or epochs\n",
    "3. **Repetitive outputs**: Adjust temperature or use beam search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "Congratulations! You've successfully trained and deployed TheBrain AI model. Here are some next steps:\n",
    "\n",
    "1. **Improve the model**:\n",
    "   - Add more training data\n",
    "   - Increase model size\n",
    "   - Try different architectures\n",
    "\n",
    "2. **Deploy the model**:\n",
    "   - Create a web API with Flask/FastAPI\n",
    "   - Build a chatbot interface\n",
    "   - Integrate into your application\n",
    "\n",
    "3. **Experiment**:\n",
    "   - Try different hyperparameters\n",
    "   - Test on different domains\n",
    "   - Implement beam search for better generation\n",
    "\n",
    "### Resources:\n",
    "- PyTorch Documentation: https://pytorch.org/docs/\n",
    "- Transformer Paper: \"Attention Is All You Need\"\n",
    "- TheBrain Repository: https://github.com/meahmedrabbi/TheBrain\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Training! \ud83d\ude80**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}